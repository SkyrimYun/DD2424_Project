# -*- coding: utf-8 -*-
"""GTZAN_MIXMATCH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P6TIgHKoUiRYVE-bVVJnDJd6mtNofxgU
"""

import pandas as pd
import numpy as np
from tqdm import trange
import os 
import librosa
import matplotlib.pyplot as plt 
import tensorflow as tf
from tensorflow.keras.backend import one_hot
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Add
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import PReLU
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.layers import GlobalMaxPooling2D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.utils import Sequence
import gc
from google.colab import drive
drive.mount('/content/drive')

#!pip3 install SpecAugment

"""## MIXMATCH"""

def add_noise(data):
  RMS=np.mean(data**2)
  n = np.random.normal(0, RMS, data.shape)
  data_noise = data+n
  return data_noise
def extract_mfcc(x,sr):
  mfccs = librosa.feature.mfcc(y=x[0], sr=sr, n_mfcc=20)
  X=mfccs
  for i in range(1,x.shape[0]):
    mfccs = librosa.feature.mfcc(y=x[i], sr=sr, n_mfcc=20)
    X = np.dstack((X,mfccs))
  return X.T
def splitSongs(X, Y, window, overlap):
  temp_X = []
  temp_Y = []
  for j in range(X.shape[0]):
    temp = X[j]
    xshape = temp.shape[0]
    chunk = int(xshape*window)
    offset = int(chunk*(1.0-overlap))
    #spltSongs=librosa.util.frame(temp, frame_length=chunk, hop_length=offset)##要求内存连续，colab会炸
    spltSongs = [temp[i:i+chunk] for i in range(0, xshape - chunk + offset, offset)]
    for s in spltSongs:      
      if s.shape[0] == chunk:
        temp_X.append(s)
        temp_Y.append(Y[j])
  #print("after",np.array(temp_X).shape)
  return np.array(temp_X), np.array(temp_Y)
def shuffle(X,Y):
  index = np.random.permutation(X.shape[0])
  Y = np.array(Y)
  return X[index],Y[index]

def argument(X):
  r=np.random.randint(0,3)
  ans = np.zeros_like(X[0])
  if r==0:
    ans=add_noise(X)
  elif r==1:
    for i in range(X.shape[0]):
      r=np.random.randint(1,3)
      X_=librosa.effects.pitch_shift(X[i], 22050, r*6)
      ans = np.dstack((ans,X_))
    ans = ans.T[1:]
    #print("ans:",ans.shape)
  else:
    for i in range(X.shape[0]):
      r=np.random.uniform(1,2)
      X_=librosa.effects.time_stretch(X[i], r)
      X_=np.pad(X_,(0,X[i].shape[0]-X_.shape[0]),"constant")
      ans = np.dstack((ans,X_))
    ans = ans.T[1:]

  return np.squeeze(ans)
def sharpen(x, T):
    temp = x**(1/T)
    return temp / np.sum(temp,axis=1,keepdims=1)
def mixup(x1, x2, y1, y2, alpha):
    beta = np.random.beta(alpha,alpha)
    beta = max(beta,1-beta)
    x = beta * x1 + (1 - beta) * x2
    y = beta * y1 + (1 - beta) * y2
    return x, y

def mixmatch(x, y, u, model, augment_fn, T=0.5, K=2, alpha=0.75):
    xb = augment_fn(x)
    ub = [augment_fn(u) for _ in range(K)]
   
    Ux = np.concatenate(ub, axis=0)
    temp = extract_mfcc(Ux,22050)
    t=model.predict_on_batch(temp)
    t = t.reshape((K,Ux.shape[0]//K,t.shape[1]))
    qb = sharpen(np.sum(t,axis=0) / K, T)

    Uy = np.concatenate([qb for _ in range(K)], axis=0)
    indices = np.random.shuffle(np.arange(len(xb) + len(Ux)))
    Wx = np.squeeze(np.concatenate([Ux, xb], axis=0)[indices])
    Wy = np.squeeze(np.concatenate([Uy, y], axis=0)[indices])

    X, p = mixup(xb, Wx[:len(xb)], y, Wy[:len(xb)], alpha)
    U, q = mixup(Ux, Wx[len(xb):], Uy, Wy[len(xb):], alpha)
    X = extract_mfcc(X,22050)
    U = extract_mfcc(U,22050)
    return X, U, p, q

raw_data = np.load('/content/drive/My Drive/Dataset/raw_data.npz')
X_raw=np.squeeze(raw_data['X_raw'])[:1000]
Y_raw=raw_data['Y_raw'][:1000]
del raw_data

X,Y=shuffle(X_raw,Y_raw)
N=X.shape[0]
X_train,Y_train = X[:3*N//5],Y[:3*N//5]
X_val,Y_val = X[3*N//5:4*N//5],Y[3*N//5:4*N//5]
X_test,Y_test = X[4*N//5:],Y[4*N//5:]
print(X_train.shape,X_val.shape,X_test.shape)
del X
del Y
gc.collect()

X_train,Y_train=splitSongs(X_train, Y_train, 0.1, 0.5)
X_val,Y_val=splitSongs(X_val, Y_val, 0.1, 0.5)
X_train=extract_mfcc(X_train,22050)
X_val=extract_mfcc(X_val,22050)
print(X_train.shape,X_val.shape)

inp_L = Input(shape=(129,20),name='labeled')
inp_U = Input(shape=(129,20),name='unlabeled')

L1 = LSTM(128,return_sequences=True)
L2 = LSTM(32)
FC = Dense(10,activation='softmax',kernel_regularizer=tf.keras.regularizers.l2(1e-4))

y_L= FC(L2(L1(inp_L)))
y_U = FC(L2(L1(inp_U)))
model_L = Model(inputs=inp_L,outputs=y_L)
model_U = Model(inputs=inp_U,outputs=y_U)
model = Model(inputs=[inp_L,inp_U],outputs=[y_L,y_U])

tf.keras.utils.plot_model(model_L, "model_L.png", show_shapes=True)
tf.keras.utils.plot_model(model_U, "model_U.png", show_shapes=True)

tf.keras.utils.plot_model(model, "model.png", show_shapes=True)
img=plt.imread("model.png")
plt.axis("off")
plt.imshow(img)

model_L.compile(loss=tf.keras.losses.categorical_crossentropy,
              metrics=['accuracy'])
model_U.compile(loss=tf.keras.losses.mse,
              metrics=['accuracy'])
model.compile(loss=[tf.keras.losses.categorical_crossentropy,tf.keras.losses.mse],
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'],loss_weights=[1,1])
print(model.summary())

def generate_data(X_L,Y_L,U_hat,X_val,Y_val,batch_size):
    i = 0
    n_it = X_L.shape[0]//batch_size
    while True:
        x,y,u = X_L[i*batch_size:i*batch_size+batch_size],Y_L[i*batch_size:i*batch_size+batch_size], U_hat[i*batch_size:i*batch_size+batch_size]
        i+=1
        X, U, p, q=mixmatch(x, y, u, model_L, argument, T=0.5, K=2, alpha=0.75)
        yield [X,U],[p,q]
        if i>=n_it:
          i=0
          X_L,Y_L=shuffle(X_L,Y_L)
          model_L.evaluate(X_val,Y_val)

batch_size =32
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath="/content/drive/My Drive/weights{epoch:02d}.h5",
    save_weights_only=True,
    save_freq="epoch"
    )
N = X_train.shape[0]
X_L,Y_L = X_train[:N//2],Y_train[:N//2]
U_hat = X_train[N//2:]
history = model.fit(generate_data(X_L,Y_L,U_hat,X_val,Y_val,batch_size),epochs=30,steps_per_epoch=X_L.shape[0]//batch_size,callbacks=model_checkpoint_callback)

data= np.load('/content/drive/My Drive/Dataset/splitted_mfccs_order0_new.npz')
X_test=data['X_test']
Y_test=data['Y_test']
model_U.load_weights("/content/drive/My Drive/weights11.h5")
model_U.evaluate(X_test,Y_test)

inp = Input(shape=(129,20))
x = LSTM(128,return_sequences=True)(inp)
x = LSTM(32)(x)
y = Dense(10,activation='softmax',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)
model_base = Model(inputs=inp,outputs=y)
print(model_base.summary())
model_base.compile(loss=tf.keras.losses.categorical_crossentropy,
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])

model_base.fit(X_train,Y_train,epochs=5,validation_data=(X_val, Y_val))

result = []
for i in range(X_test.shape[0]):
  x,y= splitSongs(X_test[i][None],Y_test[i],0.1,0.5)
  x = extract_mfcc(x,22050)
  output=np.argmax(np.bincount(np.argmax(model_base.predict(x),axis=1)))
  result.append(output==np.argmax(Y_test[i]))
print(np.sum(result)/len(result))
print(len(result))